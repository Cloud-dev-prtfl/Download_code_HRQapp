import json
import time
import re
import math
from openai import OpenAI
from django.conf import settings
from .models import ExamSession 
from django.db import close_old_connections # Import Added

# Initialize OpenAI client with Gemini Base URL
client = OpenAI(
    api_key=settings.GEMINI_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

def clean_json_string(json_string):
    """
    Sanitizes the string to handle common LLM JSON formatting errors.
    """
    if not json_string:
        return ""
    # Remove Markdown code blocks
    json_string = re.sub(r'^```json\s*', '', json_string)
    json_string = re.sub(r'^```\s*', '', json_string)
    json_string = re.sub(r'\s*```$', '', json_string)
    return json_string.strip()

def generate_questions_agent(exam_session, feedback_history=None):
    """
    Agent 1: Generates questions in BATCHES.
    Using 'gemini-2.5-flash' for SPEED.
    """
    total_needed = exam_session.num_questions
    # Batch size of 10 is safe and efficient
    BATCH_SIZE = 10 
    
    combined_questions = []
    final_title = ""
    final_summary = ""
    
    topic = exam_session.coding_languages if not exam_session.general_topic else "General Programming & Computer Science"
    
    # Calculate number of batches
    num_batches = math.ceil(total_needed / BATCH_SIZE)

    print(f"--- Starting Generation: {total_needed} questions in {num_batches} batches ---")

    for batch_idx in range(num_batches):
        # 1. Check for Cancellation
        exam_session.refresh_from_db()
        if exam_session.status == 'CANCELLED':
            return None

        # 2. Determine count for this batch
        current_count = len(combined_questions)
        remaining = total_needed - current_count
        this_batch_count = min(BATCH_SIZE, remaining)
        start_id = current_count + 1
        
        print(f"   -> Batch {batch_idx + 1}/{num_batches}: Generating {this_batch_count} questions...")

        # 3. Construct Prompt (UPDATED: Strictly MCQ)
        batch_prompt = f"""
        You are an expert Technical Interviewer.
        Generate a high-quality technical exam JSON.
        
        **CONTEXT:**
        Batch {batch_idx + 1} of {num_batches}.
        - Difficulty: {exam_session.difficulty_level}
        - Experience: {exam_session.experience_level}
        - Topic: {topic}
        - Format: MCQ
        
        **TASK:**
        Generate **{this_batch_count}** unique questions starting from ID **{start_id}**.
        
        **REQUIRED JSON STRUCTURE:**
        {{
            "exam_title": "String (Only required for Batch 1)",
            "summary": "String (Only required for Batch 1)",
            "questions": [
                {{
                    "id": {start_id}, 
                    "type": "MCQ",
                    "question_text": "String",
                    "options": ["A", "B", "C", "D"],
                    "correct_answer": "String",
                    "explanation": "String"
                }}
            ]
        }}
        """

        if feedback_history and batch_idx == 0:
            batch_prompt += f"\n\n**PREVIOUS FEEDBACK:**\n{feedback_history}"

        try:
            response = client.chat.completions.create(
                model="gemini-2.5-pro", 
                messages=[
                    {"role": "system", "content": "You are a strict JSON generator."},
                    {"role": "user", "content": batch_prompt}
                ],
                temperature=0.7,
                response_format={ "type": "json_object" },
                max_tokens=8192 
            )
            
            raw_content = response.choices[0].message.content
            cleaned_content = clean_json_string(raw_content)
            batch_data = json.loads(cleaned_content, strict=False)
            
            # Extract data
            if batch_idx == 0:
                final_title = batch_data.get("exam_title", "Technical Exam")
                final_summary = batch_data.get("summary", "Assessment generated by AI.")
            
            new_qs = batch_data.get("questions", [])
            if new_qs:
                combined_questions.extend(new_qs)

        except Exception as e:
            print(f"Agent 1 Batch Error: {e}")
            continue

    if not combined_questions:
        return None

    return {
        "exam_title": final_title,
        "summary": final_summary,
        "questions": combined_questions
    }

def evaluate_agent(questions_data, exam_session):
    """
    Agent 2: Evaluator.
    Using 'gemini-2.5-pro' for faster feedback loop.
    """
    if not questions_data:
        return 0, "No data."

    system_prompt = f"""
    Evaluate this Exam JSON.
    - Topic: {exam_session.coding_languages or "General"}
    - Difficulty: {exam_session.difficulty_level}
    - Question Count: {len(questions_data.get('questions', []))}
    
    Output JSON:
    {{
        "score": (Integer 0-100),
        "approved": (Boolean, true if score > 70),
        "feedback": "Short specific string."
    }}
    
    Data Title: {questions_data.get('exam_title')}
    """

    try:
        response = client.chat.completions.create(
            model="gemini-2.5-pro", 
            messages=[{"role": "user", "content": system_prompt}],
            temperature=0.3,
            response_format={ "type": "json_object" }
        )
        
        cleaned_content = clean_json_string(response.choices[0].message.content)
        result = json.loads(cleaned_content, strict=False)
        return result.get("score", 0), result.get("feedback", "No feedback")

    except Exception as e:
        print(f"Evaluator Error: {e}")
        return 80, "Evaluation skipped due to error."

def format_html_agent(questions_data):
    """
    Formatter: PYTHON LOGIC (Instant).
    UPDATED: Removed summary paragraph from output.
    """
    try:
        title = questions_data.get("exam_title", "Technical Exam")
        # Summary extracted but ignored for display as requested
        
        # Build Header - TITLE ONLY
        header_html = f"""
            <h2 class='exam-title'>{title}</h2>
        """

        # Build Questions List
        questions_html_parts = ['<div class="question-list">']
        
        for q in questions_data.get("questions", []):
            q_text = q.get("question_text", "")
            options = q.get("options", [])
            
            opts_html = ""
            if options:
                opts_html = '<div class="options-stack">'
                for opt in options:
                    opts_html += f'<div class="opt">{opt}</div>'
                opts_html += '</div>'
            
            item_html = f"""
                <div class="question-item">
                    <p class="q-text">{q_text}</p>
                    {opts_html}
                </div>
            """
            questions_html_parts.append(item_html)
        
        questions_html_parts.append('</div>')
        
        return {
            "header_html": header_html,
            "questions_html": "".join(questions_html_parts)
        }

    except Exception as e:
        print(f"Formatter Error: {e}")
        return {
            "header_html": "<h2>Error Formatting</h2>",
            "questions_html": "<p>Data error.</p>"
        }

def orchestrated_exam_flow(exam_session_id):
    """
    Coordinator: Manages the flow.
    """
    # [FIX]: Ensure new DB connection for this thread
    close_old_connections()
    
    try:
        session = ExamSession.objects.get(id=exam_session_id)
        session.status = 'PROCESSING'
        session.save()

        start_time = time.time()
        max_loop_duration = 600 # 10 mins max
        
        best_draft = None
        best_score = -1
        feedback = None
        attempt = 1

        print(f"--- Starting Async Flow for Session {exam_session_id} ---")

        while (time.time() - start_time) < max_loop_duration:
            session.refresh_from_db()
            if session.status == 'CANCELLED':
                print(f"Session {exam_session_id} CANCELLED.")
                return 

            print(f"[Attempt {attempt}] Generating...")
            draft = generate_questions_agent(session, feedback)
            
            if not draft:
                attempt += 1
                continue

            score, current_feedback = evaluate_agent(draft, session)
            print(f"[Attempt {attempt}] Score: {score}")

            if score > best_score:
                best_score = score
                best_draft = draft

            if score >= 85:
                break 
            
            feedback = current_feedback
            attempt += 1

        session.refresh_from_db()
        if session.status == 'CANCELLED':
            return

        if best_draft:
            formatted_data = format_html_agent(best_draft)
            session.result_html = json.dumps(formatted_data)
            session.exam_data = json.dumps(best_draft)
            session.status = 'COMPLETED'
        else:
            session.result_html = json.dumps({
                "header_html": "<h2 class='text-red-500'>Generation Failed</h2>",
                "questions_html": "<p>Could not generate valid questions.</p>"
            })
            session.status = 'FAILED'
        
        session.save()
        print(f"Session {exam_session_id} Status: {session.status}")

    except Exception as e:
        print(f"Async Error: {e}")
        try:
            # [FIX]: Ensure we can write the error state even if connection was lost
            close_old_connections()
            s = ExamSession.objects.get(id=exam_session_id)
            s.status = 'FAILED'
            s.save()
        except:
            pass